{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T10:27:38.956851Z",
     "start_time": "2020-09-03T10:26:59.475748Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:40:48.850306Z",
     "start_time": "2020-09-03T12:40:48.783364Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from collections import Counter, defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T11:41:02.315211Z",
     "start_time": "2020-09-03T11:41:02.265239Z"
    }
   },
   "outputs": [],
   "source": [
    "# datasets used\n",
    "with open('./wiki_articles/wiki_text_debugging.txt', 'r') as f:\n",
    "    article = f.read()\n",
    "    article_title = word_tokenize(article)[2]\n",
    "with open('./english_stopwords.txt') as f:\n",
    "    english_stops = f.read()\n",
    "    \n",
    "article_files = glob('./wiki_articles/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T10:35:34.764798Z",
     "start_time": "2020-09-03T10:35:34.756778Z"
    }
   },
   "source": [
    "# Word counts with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T10:39:50.766774Z",
     "start_time": "2020-09-03T10:39:50.710790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common tokens: [(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# tokenize article\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# convert tokens into lower case\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# create a counter with the lowercase tokens\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "print(f'10 most common tokens: {bow_simple.most_common()[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T10:51:35.146345Z",
     "start_time": "2020-09-03T10:51:28.098217Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\loujo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T10:57:16.210846Z",
     "start_time": "2020-09-03T10:57:16.185865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common tokens: [('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "# retain alphabetic words from lower_tokens\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# remove all stop words\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# instantiate WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize all tokens into a new list\n",
    "lemmatized = [lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# create bag-of-words\n",
    "bow = Counter(lemmatized)\n",
    "print(f'10 most common tokens: {bow.most_common(10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and querying a corpus with `gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:50:15.190127Z",
     "start_time": "2020-09-03T12:50:14.222813Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's do this preprocessing to a couple more messy articles from wikipedia\n",
    "articles = []\n",
    "\n",
    "for a in article_files:\n",
    "    #load file\n",
    "    with open(a, 'r', encoding='utf-8') as file:\n",
    "        article = file.read()\n",
    "    # tokenize words\n",
    "    tokens = word_tokenize(article)\n",
    "    # convert all to lower case\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "    # take away numeric characters\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "    # and stop words\n",
    "    no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "    \n",
    "    articles.append(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:50:15.308049Z",
     "start_time": "2020-09-03T12:50:15.190127Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 242 is computer\n",
      "\n",
      "first 10 word ids from article 5: [(1, 1), (13, 1), (15, 1), (18, 1), (26, 1), (29, 1), (37, 1), (38, 4), (47, 2), (48, 7)]\n"
     ]
    }
   ],
   "source": [
    "# create gensim Dictionary from articles\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# select the id for \"computer\"\n",
    "computer_id = dictionary.token2id.get('computer')\n",
    "\n",
    "# use computer_id with the dictionary to print the word\n",
    "print(f'ID {computer_id} is {dictionary.get(computer_id)}\\n')\n",
    "\n",
    "# create an MmCorpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# first 10 word ids with their freq counts from the fifth document\n",
    "print(f'first 10 word ids from article 5: {corpus[4][:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:12:55.956593Z",
     "start_time": "2020-09-03T12:12:55.950580Z"
    }
   },
   "source": [
    "## `gensim` bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:56:31.800645Z",
     "start_time": "2020-09-03T12:56:31.792653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 words of article 5:\n",
      "debugging 40\n",
      "system 19\n",
      "software 16\n",
      "tools 14\n",
      "computer 12\n"
     ]
    }
   ],
   "source": [
    "# save the fifth article\n",
    "doc = corpus[4]\n",
    "\n",
    "# sort the doc for frequency\n",
    "bow_doc = sorted(doc, \n",
    "                 key=lambda w: w[1], \n",
    "                 reverse=True)\n",
    "\n",
    "# print the top 5 words of the doc\n",
    "print('top 5 words of article 5:')\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:56:32.051455Z",
     "start_time": "2020-09-03T12:56:32.030467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer 598\n",
      "software 450\n",
      "cite 322\n",
      "ref 259\n",
      "code 235\n"
     ]
    }
   ],
   "source": [
    "# create a defaultdict\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# create a sorted list\n",
    "sorted_word_count = sorted(total_word_count.items(),\n",
    "                           key=lambda w: w[1],\n",
    "                           reverse=True)\n",
    "\n",
    "# top 5 words\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:33:05.340915Z",
     "start_time": "2020-09-03T12:33:05.334916Z"
    }
   },
   "source": [
    "# Tf-idf with `gensim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf with Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T12:56:33.269684Z",
     "start_time": "2020-09-03T12:56:33.173176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 weights:\n",
      "[(1, 0.012414154511302825), (13, 0.015679504267112274), (15, 0.019675969378573348), (18, 0.012414154511302825), (26, 0.019675969378573348)]\n",
      "\n",
      "top 5 weights:\n",
      "wolf 0.222521392005895\n",
      "debugging 0.20609358576129203\n",
      "fence 0.178017113604716\n",
      "debugger 0.13655569962433106\n",
      "squeeze 0.13351283520353702\n"
     ]
    }
   ],
   "source": [
    "# create a new TfidfModel using gensim's\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# calculate the tfidf weights of doc\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# first five weights\n",
    "print(f'first 5 weights:\\n{tfidf_weights[:5]}\\n')\n",
    "\n",
    "# sort\n",
    "sorted_weights = sorted(tfidf_weights,\n",
    "                        key=lambda w: w[1],\n",
    "                        reverse=True)\n",
    "\n",
    "print('top 5 weights:')\n",
    "for term_id, weight in sorted_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
