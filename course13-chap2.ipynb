{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:12:37.491077Z",
     "start_time": "2020-09-04T12:12:13.721317Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:22:33.189638Z",
     "start_time": "2020-09-04T12:22:33.185639Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:21:20.208455Z",
     "start_time": "2020-09-04T13:21:20.096259Z"
    }
   },
   "outputs": [],
   "source": [
    "#datasets used\n",
    "txt_files = glob('./*.txt')\n",
    "names = ['blog', 'gettysburg', 'lotf', 'tc']\n",
    "files = []\n",
    "for file in txt_files:\n",
    "    for name in names:\n",
    "        if name in file:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                files.append(f.read())\n",
    "\n",
    "blog, gettysburg, lotf, tc = files\n",
    "\n",
    "ted = pd.read_csv('./ted.csv')\n",
    "headlines = pd.read_csv('./fakenews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Gettysburg Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:29:56.735679Z",
     "start_time": "2020-09-04T12:29:51.859437Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the language model for spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:37:07.043897Z",
     "start_time": "2020-09-04T12:37:06.933939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\n",
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "# create a doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(f'tokens:\\n{tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing the Gettysburg address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:37:18.390364Z",
     "start_time": "2020-09-04T12:37:18.385367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\n"
     ]
    }
   ],
   "source": [
    "print(gettysburg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:39:07.973467Z",
     "start_time": "2020-09-04T12:39:07.967470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicated to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "# generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# convert lemmas into a string\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:35:28.787805Z",
     "start_time": "2020-09-04T12:35:28.780805Z"
    }
   },
   "source": [
    "# Text cleaning\n",
    "Removing...\n",
    "* Unnecessary whitespaces and escape sequences\n",
    "* Punctuations\n",
    "* Special characters (numbers, emojis, etc.)\n",
    "* Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:45:15.442332Z",
     "start_time": "2020-09-04T12:45:15.435332Z"
    }
   },
   "source": [
    "## Cleaning a blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:55:22.931387Z",
     "start_time": "2020-09-04T12:55:22.926374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe's Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\n"
     ]
    }
   ],
   "source": [
    "print(blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:55:26.018757Z",
     "start_time": "2020-09-04T12:55:25.957794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarm rise populism europe warning sign come uk brexit referendum vote swinge way leave follow stupendous victory billionaire donald trump president united states november europe steady rise populist far right party capitalize europe immigration crisis raise nationalist anti europe sentiment instance include alternative germany afd win seat enter bundestag upset germany political order time second world war success star movement italy surge popularity neo nazism neo fascism country hungary czech republic poland austria\n"
     ]
    }
   ],
   "source": [
    "# get default stopwords from spacy\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# create Doc object\n",
    "doc = nlp(blog.lower())\n",
    "\n",
    "# generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas\n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# print string after text cleaning\n",
    "print(\" \".join(a_lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the cleaned text; it is lowercased and devoid of numbers, punctuations and commonly used stopwords. Also, note that the word `U.S.` was present in the original text. Since it had periods in between, our text cleaning process completely removed it. This may not be ideal behavior. It is always advisable to use your custom functions in place of `isalpha()` for more nuanced cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning TED talks in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:59:37.000477Z",
     "start_time": "2020-09-04T12:59:36.961499Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loujo\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ted_ = ted.iloc[:20, :]\n",
    "ted_['transcript'] = ted_['transcript'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T12:59:42.411444Z",
     "start_time": "2020-09-04T12:59:42.211961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it's a great honor today to share with you the...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  we're going to talk — my — a new lecture, just...   \n",
       "1  this is a representation of your brain, and yo...   \n",
       "2  it's a great honor today to share with you the...   \n",
       "3  my passions are music, technology and making t...   \n",
       "4  it used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:02:42.029359Z",
     "start_time": "2020-09-04T13:02:42.021366Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to preprocess text\n",
    "def preprocess(text):\n",
    "    # create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # remove non-alphabetic chars\n",
    "    a_lemmas = [lemma for lemma in lemmas\n",
    "                if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return \" \".join(a_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:03:25.277995Z",
     "start_time": "2020-09-04T13:03:21.889570Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\loujo\\anaconda3\\envs\\tf2-gpu\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>talk new lecture ted illusion create ted try r...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>representation brain brain break left half log...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great honor today share digital universe creat...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>passion music technology thing combination thi...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use want computer new program programming requ...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  talk new lecture ted illusion create ted try r...   \n",
       "1  representation brain brain break left half log...   \n",
       "2  great honor today share digital universe creat...   \n",
       "3  passion music technology thing combination thi...   \n",
       "4  use want computer new program programming requ...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply preprocess to ted_['transcript']\n",
    "ted_['transcript'] = ted_['transcript'].apply(preprocess)\n",
    "ted_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging in Lord of the Flies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:11:16.990573Z",
     "start_time": "2020-09-04T13:11:16.949568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NOUN'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:10:47.392116Z",
     "start_time": "2020-09-04T13:10:47.386104Z"
    }
   },
   "source": [
    "## Counting nouns in a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:19:42.447478Z",
     "start_time": "2020-09-04T13:19:42.440483Z"
    }
   },
   "outputs": [],
   "source": [
    "# fxn returning number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "    # create Doc object\n",
    "    doc = model(text)\n",
    "    # generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "# fxn returning number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "    # create Doc object\n",
    "    doc = model(text)\n",
    "    # generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # return number of other nouns\n",
    "    return pos.count('NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:20:21.670201Z",
     "start_time": "2020-09-04T13:20:21.638238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of proper nouns in 'Abdul, Bill and Cathy went to the market to buy apples.' is 3\n",
      "\n",
      "The number of nouns in 'Abdul, Bill and Cathy went to the market to buy apples.' is 2\n"
     ]
    }
   ],
   "source": [
    "text = \"Abdul, Bill and Cathy went to the market to buy apples.\"\n",
    "\n",
    "print(f'The number of proper nouns in \\'{text}\\' is {proper_nouns(text, nlp)}\\n')\n",
    "print(f'The number of nouns in \\'{text}\\' is {nouns(text, nlp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun usage in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:21:27.041728Z",
     "start_time": "2020-09-04T13:21:26.987758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title label\n",
       "0           0                       You Can Smell Hillary’s Fear  FAKE\n",
       "1           1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n",
       "2           2        Kerry to go to Paris in gesture of sympathy  REAL\n",
       "3           3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n",
       "4           4   The Battle of New York: Why This Primary Matters  REAL"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:23:34.789703Z",
     "start_time": "2020-09-04T13:23:34.784704Z"
    }
   },
   "outputs": [],
   "source": [
    "headlines_ = headlines.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:26:09.914192Z",
     "start_time": "2020-09-04T13:26:08.618515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake news are  2.42 and  4.58, respectively.\n"
     ]
    }
   ],
   "source": [
    "headlines_['num_PROPN'] = headlines_['title'].apply(proper_nouns)\n",
    "\n",
    "# compute the mean of proper nouns\n",
    "real_propn = headlines_[headlines_['label'] == 'REAL']['num_PROPN'].mean()\n",
    "fake_propn = headlines_[headlines_['label'] == 'FAKE']['num_PROPN'].mean()\n",
    "\n",
    "print(f'Mean no. of proper nouns in real and fake news are {real_propn: .2f} and {fake_propn: .2f}, respectively.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:27:22.228984Z",
     "start_time": "2020-09-04T13:27:21.116928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of other nouns in real and fake news are  2.30 and  1.67, respectively.\n"
     ]
    }
   ],
   "source": [
    "headlines_['num_NOUN'] = headlines_['title'].apply(nouns)\n",
    "\n",
    "# compute the mean of proper nouns\n",
    "real_propn = headlines_[headlines_['label'] == 'REAL']['num_NOUN'].mean()\n",
    "fake_propn = headlines_[headlines_['label'] == 'FAKE']['num_NOUN'].mean()\n",
    "\n",
    "print(f'Mean no. of other nouns in real and fake news are {real_propn: .2f} and {fake_propn: .2f}, respectively.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:34:56.093651Z",
     "start_time": "2020-09-04T13:34:56.066666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entities and their labels:\n",
      "Sundar Pichai PERSON\n",
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "\n",
    "# create a Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# print named entities and their labels\n",
    "print('Named entities and their labels:')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:37:05.521250Z",
     "start_time": "2020-09-04T13:37:05.515277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\n"
     ]
    }
   ],
   "source": [
    "print(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:38:18.699989Z",
     "start_time": "2020-09-04T13:38:18.693993Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_persons(text):\n",
    "    # create Doc object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # identify the persons\n",
    "    persons = [ent.text for ent in doc.ents\n",
    "               if ent.label_ == 'PERSON']\n",
    "    \n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T13:39:06.405634Z",
     "start_time": "2020-09-04T13:39:06.372650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The persons in the article are: ['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "print(f'The persons in the article are: {find_persons(tc)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
