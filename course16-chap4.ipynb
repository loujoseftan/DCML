{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:08:02.500123Z",
     "start_time": "2020-09-08T17:07:41.144433Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:08:40.516735Z",
     "start_time": "2020-09-08T17:08:02.500123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Embedding, LSTM\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:29:39.989100Z",
     "start_time": "2020-09-08T10:29:39.585819Z"
    }
   },
   "outputs": [],
   "source": [
    "banknotes = pd.read_csv('./banknotes.csv')\n",
    "mnist = pd.read_csv('./mnist_train.csv', nrows=3000, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors, layers, and autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's a flow of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T08:58:35.714690Z",
     "start_time": "2020-09-08T08:58:35.389142Z"
    }
   },
   "outputs": [],
   "source": [
    "#use banknotes dataset here\n",
    "print(banknotes.shape)\n",
    "banknotes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T09:07:06.424303Z",
     "start_time": "2020-09-08T09:07:06.133511Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "X_bank = banknotes.drop('class', axis=1).to_numpy(float)\n",
    "y_bank = banknotes['class'].to_numpy(int)\n",
    "\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = train_test_split(X_bank, y_bank,\n",
    "                                                                        test_size=0.3,\n",
    "                                                                        stratify=y_bank,\n",
    "                                                                        random_state=42)\n",
    "\n",
    "# scale data\n",
    "s = StandardScaler()\n",
    "X_train_bank = s.fit_transform(X_train_bank)\n",
    "X_test_bank = s.transform(X_test_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:16:03.145736Z",
     "start_time": "2020-09-08T10:16:02.800614Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(2, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:16:03.268666Z",
     "start_time": "2020-09-08T10:16:03.255673Z"
    }
   },
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:16:03.824762Z",
     "start_time": "2020-09-08T10:16:03.797795Z"
    }
   },
   "outputs": [],
   "source": [
    "# get input tensor from 1st layer of model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# get output tensor from 1st layer of model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# print results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test_bank])[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T09:49:01.554506Z",
     "start_time": "2020-09-08T09:49:01.548506Z"
    }
   },
   "source": [
    "## Neural separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:16:11.381944Z",
     "start_time": "2020-09-08T10:16:04.686854Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for i in range(0, 21):\n",
    "    # train model for 1 epoch\n",
    "    h = model.fit(X_train_bank, y_train_bank,\n",
    "                  batch_size=16,\n",
    "                  epochs=1,\n",
    "                  verbose=0)\n",
    "    \n",
    "    if i%4==0:\n",
    "        # get output at first layer\n",
    "        layer_output = inp_to_out([X_test_bank])[0]\n",
    "        \n",
    "        # eval model acc for this epoch\n",
    "        test_acc = model.evaluate(X_test_bank, y_test_bank)[1]\n",
    "        \n",
    "        # plot 1st vs 2nd neuron output\n",
    "        axs.flatten()[i//4].scatter(layer_output[:, 0], layer_output[:, 1],\n",
    "                          c=y_test_bank, cmap='rainbow')\n",
    "        axs.flatten()[i//4].set(title=f'Epoch:{i}, Test Acc: {test_acc: .1%}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:12:41.306506Z",
     "start_time": "2020-09-08T10:12:41.301510Z"
    }
   },
   "source": [
    "If you take a look at the graphs you can see how the neurons are learning to spread out the inputs based on whether they are fake or legit dollar bills (A single fake dollar bill is represented as a purple dot in the graph). At the start the outputs are closer to each other, the weights are learned as epochs go by so that fake and legit dollar bills get a different, further and further apart output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T10:08:10.778656Z",
     "start_time": "2020-09-08T10:08:10.772659Z"
    }
   },
   "source": [
    "## Building an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T12:04:55.198736Z",
     "start_time": "2020-09-08T12:04:54.855967Z"
    }
   },
   "outputs": [],
   "source": [
    "# use mnist dataset for this one\n",
    "\n",
    "# define autoencoder arch\n",
    "autoencoder = Sequential()\n",
    "\n",
    "autoencoder.add(Dense(32, input_shape=(784,), activation='relu'))\n",
    "autoencoder.add(Dense(784, activation='sigmoid'))\n",
    "\n",
    "autoencoder.compile(optimizer=Adadelta(lr=1.0),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-noising like an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T12:04:56.564932Z",
     "start_time": "2020-09-08T12:04:56.465992Z"
    }
   },
   "outputs": [],
   "source": [
    "X_mnist = mnist.iloc[:, 1:].to_numpy(np.float32) / 255\n",
    "y_mnist = mnist.iloc[:, 0].to_numpy(np.int32)\n",
    "\n",
    "X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = train_test_split(X_mnist, y_mnist,\n",
    "                                                                            test_size=0.1,\n",
    "                                                                            stratify=y_mnist,\n",
    "                                                                            random_state=42)\n",
    "\n",
    "# add noise to test data\n",
    "noise = 0.2 * np.random.normal(size=X_test_mnist.shape)\n",
    "X_test_mnist = np.clip(X_test_mnist + noise, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T12:05:11.186155Z",
     "start_time": "2020-09-08T12:04:57.770596Z"
    }
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "early_stop = EarlyStopping(monitor='loss',\n",
    "                           patience=3)\n",
    "\n",
    "autoencoder.fit(X_train_mnist, X_train_mnist,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:45:43.111250Z",
     "start_time": "2020-09-08T13:45:43.092259Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_encodings(encodings, number):\n",
    "    '''\n",
    "    Displays first 5 encodings of noisy images from the \n",
    "    MNIST Test Data given a chosen number.\n",
    "    '''\n",
    "    idx = np.where(y_test_mnist==number)[0][:5]\n",
    "    \n",
    "    test_ims = X_test_mnist[idx]\n",
    "    encoding_ims = encodings[idx]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    \n",
    "    for ax, (t_im, e_im) in enumerate(zip(test_ims, encoding_ims)):\n",
    "        axs[0, ax].imshow(t_im.reshape(28, 28), \n",
    "                          cmap='gray')\n",
    "        axs[1, ax].imshow(np.tile(e_im, (32, 1)),\n",
    "                          cmap='gray')\n",
    "        \n",
    "        axs[0, ax].grid(False)\n",
    "        axs[0, ax].axis('off')\n",
    "        axs[1, ax].grid(False)\n",
    "        axs[1, ax].axis('off')\n",
    "    \n",
    "    axs[0, 2].set(title='Noisy Images')\n",
    "    axs[1, 2].set(title='Encodings')\n",
    "    plt.show()\n",
    "    \n",
    "def compare_ims(noisy, decoded):\n",
    "    '''\n",
    "    Display noisy and decoded images side by side\n",
    "    '''\n",
    "    idx = np.random.choice(noisy.shape[0], \n",
    "                           size=5, \n",
    "                           replace=False)\n",
    "    noisy_ims = noisy[idx]\n",
    "    decoded_ims = decoded[idx]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    \n",
    "    for ax, (n_im, d_im) in enumerate(zip(noisy_ims, decoded_ims)):\n",
    "        axs[0, ax].imshow(n_im.reshape(28, 28), \n",
    "                          cmap='gray')\n",
    "        axs[1, ax].imshow(d_im.reshape(28, 28),\n",
    "                          cmap='gray')\n",
    "                          \n",
    "        axs[0, ax].grid(False)\n",
    "        axs[0, ax].axis('off')\n",
    "        axs[1, ax].grid(False)\n",
    "        axs[1, ax].axis('off')\n",
    "                          \n",
    "    axs[0, 2].set(title='Noisy Images')\n",
    "    axs[1, 2].set(title='Decoded Images')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:36:17.132748Z",
     "start_time": "2020-09-08T13:36:15.015183Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract encoder of autoencoder thru its first layer\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# encode noisy images and show the encodings\n",
    "encodings = encoder.predict(X_test_mnist)\n",
    "show_encodings(encodings=encodings,\n",
    "               number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:47:19.431717Z",
     "start_time": "2020-09-08T13:47:17.658869Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict on noisy images with autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_mnist)\n",
    "\n",
    "compare_ims(X_test_mnist, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:22:28.724581Z",
     "start_time": "2020-09-08T13:22:28.718587Z"
    }
   },
   "source": [
    "# Intro to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:10.214200Z",
     "start_time": "2020-09-08T14:47:09.775733Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model arch\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, \n",
    "                 kernel_size=3,\n",
    "                 input_shape=(28, 28, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(16,\n",
    "                 kernel_size=3,\n",
    "                 activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:10.896225Z",
     "start_time": "2020-09-08T14:47:10.888229Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:15.194119Z",
     "start_time": "2020-09-08T14:47:13.994333Z"
    }
   },
   "outputs": [],
   "source": [
    "# i'll just use the test data bc 60k is a lot for this simple case\n",
    "(_, _), (X_mnist2d, y_mnist2d) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:15.629454Z",
     "start_time": "2020-09-08T14:47:15.578463Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "X_mnist2d = X_mnist2d.astype('float32') / 255\n",
    "X_mnist2d = np.expand_dims(X_mnist2d, -1) # make sure input shape is (28, 28, 1)\n",
    "\n",
    "y_mnist2d = to_categorical(y_mnist2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:16.731013Z",
     "start_time": "2020-09-08T14:47:16.502290Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tr_m2d, X_tt_m2d, y_tr_m2d, y_tt_m2d = train_test_split(X_mnist2d, y_mnist2d,\n",
    "                                                          test_size=0.1,\n",
    "                                                          stratify=y_mnist2d,\n",
    "                                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:18.177978Z",
     "start_time": "2020-09-08T14:47:18.172982Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{X_tr_m2d.shape[0]} train samples with dims {X_tr_m2d.shape[1:]}')\n",
    "print(f'{X_tt_m2d.shape[0]} test samples with dims {X_tt_m2d.shape[1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:47:53.138000Z",
     "start_time": "2020-09-08T14:47:26.227044Z"
    }
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=2)\n",
    "\n",
    "model.fit(X_tr_m2d, y_tr_m2d,\n",
    "          epochs=1000,\n",
    "          batch_size=256,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[early_stop],\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:41:41.683671Z",
     "start_time": "2020-09-08T14:41:41.676674Z"
    }
   },
   "source": [
    "## Looking at convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:53:00.371612Z",
     "start_time": "2020-09-08T14:52:59.574667Z"
    }
   },
   "outputs": [],
   "source": [
    "# obtain a reference to the outputs of the first layer\n",
    "layer1_out = model.layers[0].output\n",
    "\n",
    "# build a model using the model's input and first layer output\n",
    "layer1_model = Model(inputs=model.layers[0].input,\n",
    "                     outputs=layer1_out)\n",
    "\n",
    "# use this model to pred on test data\n",
    "activations = layer1_model.predict(X_tt_m2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T14:58:14.246371Z",
     "start_time": "2020-09-08T14:58:13.841113Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "ax1.matshow(activations[0, :, :, 14],\n",
    "            cmap='viridis')\n",
    "ax2.matshow(activations[0, :, :, 17],\n",
    "            cmap='viridis')\n",
    "ax1.grid(False)\n",
    "ax2.grid(False)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "ax1.set(title='Conv at 15th filter')\n",
    "ax2.set(title='Conv at 18th filter')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T15:13:59.846051Z",
     "start_time": "2020-09-08T15:13:59.841055Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will use a pretrained resnet50 here\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T15:14:02.269962Z",
     "start_time": "2020-09-08T15:14:01.748511Z"
    }
   },
   "outputs": [],
   "source": [
    "# load image with the right target size for model\n",
    "img = image.load_img('./naz.jpg', target_size=(224, 224))\n",
    "\n",
    "# turn im into array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "plt.imshow(img_array / 255)\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "plt.title('Our dog Naz')\n",
    "plt.show()\n",
    "\n",
    "# expand dims of image\n",
    "img_expanded = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# pre-process img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T15:12:59.714785Z",
     "start_time": "2020-09-08T15:12:59.707769Z"
    }
   },
   "source": [
    "## Using a real world model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T15:44:33.788416Z",
     "start_time": "2020-09-08T15:44:27.496589Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# predict with ResNet50 on prepped img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# decode first 3 preds\n",
    "print(f'Predicted: {decode_predictions(preds, top=3)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naz is a Labrador Retriever but I searched and he does look like a Great Pyrenees on some angles. Cute dogssssss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text predictions with LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:09:12.182354Z",
     "start_time": "2020-09-08T17:09:12.177358Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'it is not the strength of the body but the strength of the spirit it is useless to meet revenge with revenge it will heal nothing even the smallest person can change the course of history all we have to decide is what to do with the time that is given us the burned hand teaches best after that advice about fire goes to the heart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:09:12.782450Z",
     "start_time": "2020-09-08T17:09:12.768458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "['it is not the', 'is not the strength', 'not the strength of', 'the strength of the', 'strength of the body']\n",
      "Sequences:\n",
      "[[5, 2, 42, 1], [2, 42, 1, 6], [42, 1, 6, 4], [1, 6, 4, 1], [6, 4, 1, 10]]\n"
     ]
    }
   ],
   "source": [
    "# split text into an array of words\n",
    "words = text.split()\n",
    "\n",
    "# make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "    sentences.append(' '.join(words[i-4: i]))\n",
    "    \n",
    "# instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(f'Sentences:\\n{sentences[:5]}\\nSequences:\\n{sequences[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:09:18.566937Z",
     "start_time": "2020-09-08T17:09:18.562939Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.index_word) + 1 # account for the 0th index (special chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:19:39.586283Z",
     "start_time": "2020-09-08T17:19:38.767278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 3, 8)              352       \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                5248      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 44)                1452      \n",
      "=================================================================\n",
      "Total params: 8,108\n",
      "Trainable params: 8,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, input_length=3, output_dim=8)) # turn words into vectors\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(0.1),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:19:40.192274Z",
     "start_time": "2020-09-08T17:19:40.186274Z"
    }
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in range(len(sequences)):\n",
    "    X.append(sequences[i][:3])\n",
    "    y.append(sequences[i][3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = to_categorical(y, 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:19:44.696717Z",
     "start_time": "2020-09-08T17:19:40.785354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62 samples\n",
      "Epoch 1/100\n",
      "62/62 - 2s - loss: 3.8315 - accuracy: 0.0161\n",
      "Epoch 2/100\n",
      "62/62 - 0s - loss: 3.7939 - accuracy: 0.1129\n",
      "Epoch 3/100\n",
      "62/62 - 0s - loss: 3.4509 - accuracy: 0.1452\n",
      "Epoch 4/100\n",
      "62/62 - 0s - loss: 3.1817 - accuracy: 0.1452\n",
      "Epoch 5/100\n",
      "62/62 - 0s - loss: 2.7797 - accuracy: 0.2097\n",
      "Epoch 6/100\n",
      "62/62 - 0s - loss: 2.5463 - accuracy: 0.2258\n",
      "Epoch 7/100\n",
      "62/62 - 0s - loss: 2.3511 - accuracy: 0.2581\n",
      "Epoch 8/100\n",
      "62/62 - 0s - loss: 1.9126 - accuracy: 0.3065\n",
      "Epoch 9/100\n",
      "62/62 - 0s - loss: 1.6205 - accuracy: 0.4839\n",
      "Epoch 10/100\n",
      "62/62 - 0s - loss: 1.2067 - accuracy: 0.5645\n",
      "Epoch 11/100\n",
      "62/62 - 0s - loss: 1.0957 - accuracy: 0.5968\n",
      "Epoch 12/100\n",
      "62/62 - 0s - loss: 0.6913 - accuracy: 0.7581\n",
      "Epoch 13/100\n",
      "62/62 - 0s - loss: 0.5514 - accuracy: 0.8226\n",
      "Epoch 14/100\n",
      "62/62 - 0s - loss: 0.4032 - accuracy: 0.8387\n",
      "Epoch 15/100\n",
      "62/62 - 0s - loss: 0.3237 - accuracy: 0.8548\n",
      "Epoch 16/100\n",
      "62/62 - 0s - loss: 0.1854 - accuracy: 0.9516\n",
      "Epoch 17/100\n",
      "62/62 - 0s - loss: 0.0960 - accuracy: 0.9839\n",
      "Epoch 18/100\n",
      "62/62 - 0s - loss: 0.0812 - accuracy: 0.9839\n",
      "Epoch 19/100\n",
      "62/62 - 0s - loss: 0.0560 - accuracy: 0.9839\n",
      "Epoch 20/100\n",
      "62/62 - 0s - loss: 0.0543 - accuracy: 0.9677\n",
      "Epoch 21/100\n",
      "62/62 - 0s - loss: 0.0388 - accuracy: 0.9839\n",
      "Epoch 22/100\n",
      "62/62 - 0s - loss: 0.0391 - accuracy: 0.9839\n",
      "Epoch 23/100\n",
      "62/62 - 0s - loss: 0.0298 - accuracy: 0.9839\n",
      "Epoch 24/100\n",
      "62/62 - 0s - loss: 0.0281 - accuracy: 0.9839\n",
      "Epoch 25/100\n",
      "62/62 - 0s - loss: 0.0294 - accuracy: 0.9839\n",
      "Epoch 26/100\n",
      "62/62 - 0s - loss: 0.0242 - accuracy: 0.9839\n",
      "Epoch 27/100\n",
      "62/62 - 0s - loss: 0.0280 - accuracy: 0.9839\n",
      "Epoch 28/100\n",
      "62/62 - 0s - loss: 0.0279 - accuracy: 0.9677\n",
      "Epoch 29/100\n",
      "62/62 - 0s - loss: 0.0232 - accuracy: 0.9839\n",
      "Epoch 30/100\n",
      "62/62 - 0s - loss: 0.0269 - accuracy: 0.9839\n",
      "Epoch 31/100\n",
      "62/62 - 0s - loss: 0.0286 - accuracy: 0.9677\n",
      "Epoch 32/100\n",
      "62/62 - 0s - loss: 0.0236 - accuracy: 0.9839\n",
      "Epoch 33/100\n",
      "62/62 - 0s - loss: 0.0268 - accuracy: 0.9677\n",
      "Epoch 34/100\n",
      "62/62 - 0s - loss: 0.0231 - accuracy: 0.9839\n",
      "Epoch 35/100\n",
      "62/62 - 0s - loss: 0.0261 - accuracy: 0.9677\n",
      "Epoch 36/100\n",
      "62/62 - 0s - loss: 0.0230 - accuracy: 0.9839\n",
      "Epoch 37/100\n",
      "62/62 - 0s - loss: 0.0229 - accuracy: 0.9839\n",
      "Epoch 38/100\n",
      "62/62 - 0s - loss: 0.0248 - accuracy: 0.9677\n",
      "Epoch 39/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 40/100\n",
      "62/62 - 0s - loss: 0.0250 - accuracy: 0.9677\n",
      "Epoch 41/100\n",
      "62/62 - 0s - loss: 0.0264 - accuracy: 0.9839\n",
      "Epoch 42/100\n",
      "62/62 - 0s - loss: 0.0232 - accuracy: 0.9839\n",
      "Epoch 43/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 44/100\n",
      "62/62 - 0s - loss: 0.0276 - accuracy: 0.9839\n",
      "Epoch 45/100\n",
      "62/62 - 0s - loss: 0.0262 - accuracy: 0.9839\n",
      "Epoch 46/100\n",
      "62/62 - 0s - loss: 0.0231 - accuracy: 0.9839\n",
      "Epoch 47/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 48/100\n",
      "62/62 - 0s - loss: 0.0246 - accuracy: 0.9839\n",
      "Epoch 49/100\n",
      "62/62 - 0s - loss: 0.0275 - accuracy: 0.9839\n",
      "Epoch 50/100\n",
      "62/62 - 0s - loss: 0.0236 - accuracy: 0.9839\n",
      "Epoch 51/100\n",
      "62/62 - 0s - loss: 0.0238 - accuracy: 0.9839\n",
      "Epoch 52/100\n",
      "62/62 - 0s - loss: 0.0230 - accuracy: 0.9839\n",
      "Epoch 53/100\n",
      "62/62 - 0s - loss: 0.0237 - accuracy: 0.9839\n",
      "Epoch 54/100\n",
      "62/62 - 0s - loss: 0.0233 - accuracy: 0.9839\n",
      "Epoch 55/100\n",
      "62/62 - 0s - loss: 0.0252 - accuracy: 0.9839\n",
      "Epoch 56/100\n",
      "62/62 - 0s - loss: 0.0229 - accuracy: 0.9839\n",
      "Epoch 57/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 58/100\n",
      "62/62 - 0s - loss: 0.0241 - accuracy: 0.9677\n",
      "Epoch 59/100\n",
      "62/62 - 0s - loss: 0.0261 - accuracy: 0.9677\n",
      "Epoch 60/100\n",
      "62/62 - 0s - loss: 0.0253 - accuracy: 0.9839\n",
      "Epoch 61/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 62/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 63/100\n",
      "62/62 - 0s - loss: 0.0250 - accuracy: 0.9839\n",
      "Epoch 64/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 65/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 66/100\n",
      "62/62 - 0s - loss: 0.0245 - accuracy: 0.9677\n",
      "Epoch 67/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 68/100\n",
      "62/62 - 0s - loss: 0.0253 - accuracy: 0.9839\n",
      "Epoch 69/100\n",
      "62/62 - 0s - loss: 0.0244 - accuracy: 0.9839\n",
      "Epoch 70/100\n",
      "62/62 - 0s - loss: 0.0266 - accuracy: 0.9677\n",
      "Epoch 71/100\n",
      "62/62 - 0s - loss: 0.0234 - accuracy: 0.9839\n",
      "Epoch 72/100\n",
      "62/62 - 0s - loss: 0.0251 - accuracy: 0.9839\n",
      "Epoch 73/100\n",
      "62/62 - 0s - loss: 0.0234 - accuracy: 0.9839\n",
      "Epoch 74/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 75/100\n",
      "62/62 - 0s - loss: 0.0264 - accuracy: 0.9839\n",
      "Epoch 76/100\n",
      "62/62 - 0s - loss: 0.0244 - accuracy: 0.9839\n",
      "Epoch 77/100\n",
      "62/62 - 0s - loss: 0.0234 - accuracy: 0.9839\n",
      "Epoch 78/100\n",
      "62/62 - 0s - loss: 0.0255 - accuracy: 0.9677\n",
      "Epoch 79/100\n",
      "62/62 - 0s - loss: 0.0242 - accuracy: 0.9839\n",
      "Epoch 80/100\n",
      "62/62 - 0s - loss: 0.0231 - accuracy: 0.9839\n",
      "Epoch 81/100\n",
      "62/62 - 0s - loss: 0.0231 - accuracy: 0.9839\n",
      "Epoch 82/100\n",
      "62/62 - 0s - loss: 0.0241 - accuracy: 0.9839\n",
      "Epoch 83/100\n",
      "62/62 - 0s - loss: 0.0252 - accuracy: 0.9677\n",
      "Epoch 84/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 85/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 86/100\n",
      "62/62 - 0s - loss: 0.0227 - accuracy: 0.9839\n",
      "Epoch 87/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 88/100\n",
      "62/62 - 0s - loss: 0.0250 - accuracy: 0.9839\n",
      "Epoch 89/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 90/100\n",
      "62/62 - 0s - loss: 0.0225 - accuracy: 0.9839\n",
      "Epoch 91/100\n",
      "62/62 - 0s - loss: 0.0225 - accuracy: 0.9839\n",
      "Epoch 92/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 93/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n",
      "Epoch 94/100\n",
      "62/62 - 0s - loss: 0.0243 - accuracy: 0.9839\n",
      "Epoch 95/100\n",
      "62/62 - 0s - loss: 0.0225 - accuracy: 0.9839\n",
      "Epoch 96/100\n",
      "62/62 - 0s - loss: 0.0225 - accuracy: 0.9839\n",
      "Epoch 97/100\n",
      "62/62 - 0s - loss: 0.0228 - accuracy: 0.9839\n",
      "Epoch 98/100\n",
      "62/62 - 0s - loss: 0.0253 - accuracy: 0.9677\n",
      "Epoch 99/100\n",
      "62/62 - 0s - loss: 0.0247 - accuracy: 0.9839\n",
      "Epoch 100/100\n",
      "62/62 - 0s - loss: 0.0226 - accuracy: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a479901148>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,\n",
    "          epochs=100,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:23:09.719303Z",
     "start_time": "2020-09-08T17:23:09.703676Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_text(test_text, model=model):\n",
    "    if len(test_text.split()) != 3:\n",
    "        print('Text input should be 3 words!')\n",
    "        return False\n",
    "    \n",
    "    # Turn test_text into sequence of numbers\n",
    "    test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "    test_seq = np.array(test_seq)\n",
    "    \n",
    "    # use model passed as a parameter to predict the next word\n",
    "    pred = model.predict(test_seq).argmax(axis=1)[0]\n",
    "    \n",
    "    # return word that maps to the prediction\n",
    "    return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T17:23:52.230162Z",
     "start_time": "2020-09-08T17:23:52.182187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spirit'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_text('strength of the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-gpu",
   "language": "python",
   "name": "tf2-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
